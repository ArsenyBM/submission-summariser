{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbdc0aa",
   "metadata": {},
   "source": [
    "# Submission summariser\n",
    "\n",
    "The submission summariser extracts, parses, and summarises text from PDF submissions using Python and generative AI. This ongoing project involves processing PDF submissions, extracting text, cleaning the data, and using AI-powered models like GPT-3 or GPT-4 to provide summaries and insights from the submissions. The generated summaries are then stored in an SQLite database and can be visualized through WordCloud, bar charts, or exported to Excel sheets for further analysis.\n",
    "\n",
    "**Note**: Explanations in this doc are still in rough draft and will be polished/clarified in subsequent drafts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bf806",
   "metadata": {},
   "source": [
    "## Preliminary steps\n",
    "### Installing relevant package\n",
    "Install necessary packages by running the following command in your Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber openai pandas tiktoken dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5ebfc",
   "metadata": {},
   "source": [
    "### Downloading and preparing submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b2a39",
   "metadata": {},
   "source": [
    "Manually download submissions and save them in one folder on your machine. \n",
    "\n",
    "**Recommended step**: Preprocess larger PDFs (by word count/page count or by size of file) to remove attachments/material that are not necessary for understanding the submission - this can help control cost and, more importantly, help to fit the submission within generative AI context windows (i.e. AI word limits)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e82fb",
   "metadata": {},
   "source": [
    "## Step one: text extraction\n",
    "In the first step, the code imports necessary modules and functions, connects to the `submission_data.db` SQLite database, and creates a table called `submissions`. It then processes PDF files, extracts text, counts pages, words, and tokens, and stores the data in the `submissions` table. The table will contain columns for file_name, page_count, word_count, text, tokens, and manual_summarisation.\n",
    "\n",
    "A helper function `remove_illegal_chars()` removes any illegal characters in the extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7485b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import tiktoken\n",
    "import sqlite3\n",
    "\n",
    "# Directory with PDF files\n",
    "pdf_dir = \"/users/arseny/Desktop/testsubmissions\" #This is the default that works on Arseny's computer - on other computers, you will be prompted to provide a directory.\n",
    "while not os.path.exists(pdf_dir):\n",
    "    pdf_dir = input(\"Enter the directory where submissions are saved or 'quit' to exit: \")\n",
    "    \n",
    "    if pdf_dir.lower() == 'quit':\n",
    "        print(\"You chose to exit. Program will terminate.\")\n",
    "        quit()\n",
    "        \n",
    "    elif not os.path.exists(pdf_dir):\n",
    "        print(f\"Directory does not exist: {pdf_dir}\")\n",
    "\n",
    "# Token counter\n",
    "encoding = tiktoken.encoding_for_model('gpt-4') # Replace with other model if not using GPT-4\n",
    "\n",
    "# SQLite connection\n",
    "\n",
    "\n",
    "try:\n",
    "    db_name\n",
    "except NameError:\n",
    "    db_name = input(\"Please enter the name for your database (exclude '.db'): \") + '.db'\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS submissions\n",
    "    (file_name TEXT PRIMARY KEY, page_count INTEGER, word_count INTEGER, text TEXT, tokens INTEGER, manual_summarisation INTEGER DEFAULT 0)\n",
    "''')\n",
    "\n",
    "# Function to remove illegal characters\n",
    "def remove_illegal_chars(text):\n",
    "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "    text = ILLEGAL_CHARACTERS_RE.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "    # Loop over the PDF files in the directory\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_dir, file_name)\n",
    "        \n",
    "        # Check if file exists in database\n",
    "        c.execute(\"SELECT file_name FROM submissions WHERE file_name = ?\", (file_name,))\n",
    "        if c.fetchone() is None:\n",
    "            \n",
    "            # Open the PDF file\n",
    "            with pdfplumber.open(file_path) as pdf_file:\n",
    "                \n",
    "                # Initialize an empty string for the text\n",
    "                text = ''\n",
    "\n",
    "                # Loop over the pages and add the text to the string\n",
    "                for i,  page in enumerate(pdf_file.pages):\n",
    "                    try:\n",
    "                        page_text = page.extract_text()\n",
    "                        page_text = remove_illegal_chars(page_text)\n",
    "                        text += page_text\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"For {file_name},failed to extract text from page {i} with error: {str(e)}\")\n",
    "            \n",
    "                # Count the pages\n",
    "                page_count = len(pdf_file.pages)\n",
    "            \n",
    "                # Count the words\n",
    "                word_count = len(text.split())\n",
    "            \n",
    "                # Number of tokens\n",
    "                num_tokens = len(encoding.encode(text))\n",
    "  \n",
    "                # Insert into database\n",
    "                c.execute(\"INSERT INTO submissions VALUES (?, ?, ?, ?, ?, 0)\",\n",
    "                          (file_name, page_count, word_count, text, num_tokens))\n",
    "            \n",
    "# Commit the changes and close the connection to the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736e640",
   "metadata": {},
   "source": [
    "## Step two: 'cleaning' the data\n",
    "There might be some issues with the text extraction process, such as getting nonsensical or header/footer text from the PDFs or failing to extract text at all. It is planned to filter for such cases either by manual inspection of the extracted text or by creating a partially automated approach to filter based on the number of words extracted, and by using an LLM (laguage model) to categorize the contents as either actual text or nonsense/header-footer text.\n",
    "\n",
    "This step will help in filtering out the submissions that need manual summarisation.\n",
    "\n",
    "**Update 13/7/23**: The interim solution I've arrived at it is using dedicated OCR technology for problematic submissions once I've identified them manually inspecting the database after text extraction. I've used Abby Finereader (which requires a subscription) to extact OCR PDFs and export the extracted text to a .txt file. The code below reads the text files into the database as separate entries (note: page count doesn't work with .txt files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_name\n",
    "except NameError:\n",
    "    db_name = input(\"Please enter the name for your database (exclude '.db'): \") + '.db'\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS submissions\n",
    "    (file_name TEXT PRIMARY KEY, page_count INTEGER, word_count INTEGER, text TEXT, tokens INTEGER, manual_summarisation INTEGER DEFAULT 0)\n",
    "''')\n",
    "\n",
    "# Loop over the TXT files in the directory\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(pdf_dir, file_name)\n",
    "        \n",
    "        # Check if file exists in database\n",
    "        c.execute(\"SELECT file_name FROM submissions WHERE file_name = ?\", (file_name,))\n",
    "        if c.fetchone() is None:\n",
    "            \n",
    "            # Open the TXT file and read it\n",
    "            with open(file_path, 'r') as txt_file:\n",
    "                text = txt_file.read()\n",
    "                text = remove_illegal_chars(text)\n",
    "            \n",
    "            # Count the words\n",
    "            word_count = len(text.split())\n",
    "            \n",
    "            # Number of tokens\n",
    "            num_tokens = len(encoding.encode(text))\n",
    "            \n",
    "            # We can't get the number of pages from a TXT file, so we set it to None\n",
    "            page_count = None\n",
    "\n",
    "            # Insert into database\n",
    "            c.execute(\"INSERT INTO submissions VALUES (?, ?, ?, ?, ?, 0)\",\n",
    "                      (file_name, page_count, word_count, text, num_tokens))\n",
    "            \n",
    "# Commit the changes and close the connection to the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859eea9",
   "metadata": {},
   "source": [
    "## Step three: summarisation\n",
    "In the third step, the code calls the OpenAI API to summarize the text. The summarization requires having an OpenAI API key saved in an environment file. The summarization process takes into account the extracted text from the submissions and selects an appropriate GPT model based on the number of tokens. \n",
    "\n",
    "Using the OpenAI LLM, the code summarizes the text, extracts key policy ideas and concerns, categorizes the author as 'individual', 'government', 'business', 'other', or 'unsure', and rates the sentiment of the text towards Government's current policies. Finally, it stores the summarized results in a new table named `summaries`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"constants.env\")) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Create summariser function\n",
    "\n",
    "def get_completion(prompt, model): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "# Check if db_name is stored, if not, prompt the user to enter it\n",
    "try:\n",
    "    db_name\n",
    "except NameError:\n",
    "    db_name = input(\"Please enter the name of your database (exclude '.db'): \") + '.db'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_name)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Load the data from the database\n",
    "c.execute(\"SELECT * FROM submissions\")\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Create a new table for the summaries if it doesn't exist\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS summaries\n",
    "    (file_name TEXT PRIMARY KEY, summary TEXT, ideas TEXT, concerns TEXT, \n",
    "     category TEXT, sentiment TEXT, page_count INTEGER, text TEXT, \n",
    "     manual_summarisation INTEGER DEFAULT 0, model_used TEXT, LLM_response TEXT)\n",
    "''')\n",
    "\n",
    "# Loop over the rows in the fetched data\n",
    "for row in rows:\n",
    "    file_name, page_count, word_count, text, tokens, manual_summarisation = row\n",
    "\n",
    "    # Check if summary already exists in the database\n",
    "    c.execute(\"SELECT file_name FROM summaries WHERE file_name = ?\", (file_name,))\n",
    "    if c.fetchone() is not None:\n",
    "        continue  # Skip this row if it's already in the database\n",
    "        \n",
    "    # Check for manual summarisation requirement\n",
    "    if manual_summarisation == 1:\n",
    "        model_used = \"Manual summarisation required\"\n",
    "        summary = ideas = concerns = category = sentiment = text = model = response = \"Manual summarisation required\"\n",
    "        c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 1, ?, ?)\",\n",
    "            (file_name, summary, ideas, concerns, category, sentiment, page_count, text, model, response))\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Determine the model to use based on the number of tokens\n",
    "        model = \"gpt-3.5-turbo-16k-0613\" if tokens > 7100 else \"gpt-4-0613\"\n",
    "    \n",
    "        # Summarize the text - check prompt library for alternatives\n",
    "        prompt_itself = f\"\"\"\n",
    "        Your task is to perform the following actions as if you are a highly experienced, knowledgeable Australian Government policy adviser who is known for their clear thinking and writing:\n",
    "        1. Summarise the following public submission extracted below. Make sure to highlight any key points raised in the submission about whether the proposed acquisition would substantially lessen competition in the markets for the following products: home loans; transaction accounts; deposit products; agribusiness banking; SME banking.\n",
    "        2. Extract the key areas of support for the acquisition, if any, raised in the submission.\n",
    "        3. Extract the key concerns about the acquisition, if any, raised in the submission.\n",
    "        4. Categorise the author as 'individual' (including more than one individual), 'government', 'business', 'other' or 'unsure'. No commentary, just the category. \n",
    "        5. On a scale of 1 to 10, rank the sentiment of the text towards whether the proposed acquisition should be authorised (1 being very negative and 10 being very positive). No need for commentary, just the number.\n",
    "        6. Output valid JSON that contains the following keys corresponding to the above tasks: summary, ideas, concerns, category, sentiment.\n",
    "        Only produce output for step 6 - the JSON output should incorporate the previous answers.\n",
    "\n",
    "               \n",
    "        Here is the submission text:\"\"\"\n",
    "        \n",
    "        chunk_prompt = f\"\"\"\n",
    "        You have been given a chunk from a longer document that exceeds token limits.\n",
    "        Your task is to extract and summarise useful information that, when combined with summaries of other chunks, will allow you to perform the following task. You must output in string format and finish the summary with the following \"End of chunk.\".\n",
    "        The following is the prompt you would have received if the document was within token limits as well as an extract of the relevant chunk:\"\"\"\n",
    "        \n",
    "        combined_summary_prompt = f\"\"\"\n",
    "        Your task is to use the following text which is a collection of summarised chunks from a longer document to complete the following task.\n",
    "        The reason the document has been chunked is because it exceeded token limits.\n",
    "        The following is the prompt you would have received if the document was within token limits as well as the combined summarised chunks:\"\"\"\n",
    "\n",
    "        try:\n",
    "            if tokens < 15000:  # 15,000 to fit within token limit on GPT-3.5 16k\n",
    "                prompt = prompt_itself + text\n",
    "                response = get_completion(prompt, model)\n",
    "            \n",
    "            else:\n",
    "                # Initialise the tokenizer\n",
    "                tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "                chunk_summaries = \"\"\n",
    "                \n",
    "                # Chunk up the document into 10,000 token chunks\n",
    "                chunks = create_chunks(text, 10000, tokenizer)\n",
    "                text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "                print(\"Summarizing each chunk of text\")\n",
    "                \n",
    "                # Summarise each chunk\n",
    "                for chunk in text_chunks:\n",
    "                    prompt = chunk_prompt + prompt_itself + chunk\n",
    "                    response = get_completion(prompt, model)\n",
    "                    chunk_summaries += chunk_summaries\n",
    "                \n",
    "                # Final summary\n",
    "                print(\"Summarizing into overall summary\")\n",
    "                prompt = combined_summary_prompt + prompt_itself + chunk_summaries\n",
    "                response = get_completion(prompt, model)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            print(f\"Error for file {file_name}: {error_message}\")\n",
    "            c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)\",\n",
    "                  (file_name, 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', page_count, text, model, error_message))\n",
    "            continue\n",
    "\n",
    "        # Parse the response as JSON\n",
    "        try:\n",
    "            response_json = json.loads(response)\n",
    "            # Extract the fields\n",
    "            summary = response_json.get('summary', 'Error: could not parse response')\n",
    "            ideas = json.dumps(response_json.get('ideas', 'Error: could not parse response'))\n",
    "            concerns = json.dumps(response_json.get('concerns', 'Error: could not parse response'))\n",
    "            category = response_json.get('category', 'Error: could not parse response')\n",
    "            sentiment = response_json.get('sentiment', 'Error: could not parse response')\n",
    "        except json.JSONDecodeError:\n",
    "            \n",
    "            # If parsing fails, ask LLM to convert to JSON\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "                Your task is to flawlessly convert the following input to valid JSON that contains the following keys: summary, ideas, concerns, category, sentiment.\n",
    "                Here is the input you must convert to valid JSON: {response}\"\"\"\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                print(f\"Error for file {file_name}: {error_message}\")\n",
    "                c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)\",\n",
    "                    (file_name, 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', page_count, text, model, error_message))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = get_completion(prompt, model)\n",
    "                \n",
    "                # Extract the fields\n",
    "                summary = response_json.get('summary', 'Error: could not parse response')\n",
    "                ideas = json.dumps(response_json.get('ideas', 'Error: could not parse response'))\n",
    "                concerns = json.dumps(response_json.get('concerns', 'Error: could not parse response'))\n",
    "                category = response_json.get('category', 'Error: could not parse response')\n",
    "                sentiment = response_json.get('sentiment', 'Error: could not parse response')\n",
    "            except json.JSONDecodeError:\n",
    "                 summary = ideas = concerns = category = sentiment = 'Error: could not parse response'\n",
    "                       \n",
    "        # Insert into database\n",
    "        c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)\",\n",
    "            (file_name, summary, ideas, concerns, category, sentiment, page_count, text, model, response))\n",
    "    \n",
    "        print(f'Summarised and stored summary for: {file_name}')\n",
    "\n",
    "# Commit the changes and close the connection to the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2978734b",
   "metadata": {},
   "source": [
    "## Step four: formatting output\n",
    "The fourth step involves cleaning the output data further, such as converting lists of ideas and concerns to bullet point lists. It then writes the resulting data to an Excel file, `summaries_and_submissions.xlsx`, for easy examination of the summaries generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import ast\n",
    "\n",
    "# Connect to the SQLite database\n",
    "try:\n",
    "    db_name\n",
    "except NameError:\n",
    "    db_name = input(\"Please enter the name of your database (exclude '.db'): \") + '.db'\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# Write the query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    s.file_name, \n",
    "    s.summary, \n",
    "    s.ideas, \n",
    "    s.concerns, \n",
    "    s.category, \n",
    "    s.sentiment, \n",
    "    s.page_count, \n",
    "    sub.word_count, \n",
    "    sub.tokens, \n",
    "    s.manual_summarisation, \n",
    "    s.model_used, \n",
    "    s.LLM_response\n",
    "FROM summaries as s\n",
    "INNER JOIN submissions as sub\n",
    "ON s.file_name = sub.file_name\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and store the result in a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Filter out rows that require manual summarisation\n",
    "df = df[df['manual_summarisation'] == 0]\n",
    "\n",
    "#Filter out rows that LLM didn't process\n",
    "df = df[df['category'] != 'Error: Could not generate OpenAI response']\n",
    "\n",
    "\n",
    "# Code below is a bit messy but seems to work - intention is to convert lists to bullet points\n",
    "# To-do: come back to this code and make neater\n",
    "\n",
    "def bullet_list_or_string(data_str):\n",
    "    try:\n",
    "        data = ast.literal_eval(data_str)\n",
    "        if isinstance(data, list):\n",
    "            if len(data) > 1:  # If data contains more than one item, convert to bullet list\n",
    "                return \"\\n\".join(f'â€¢ {item}' for item in data)\n",
    "            elif len(data) == 1:  # If data is a single item, return as-is\n",
    "                return data[0]\n",
    "            else:  # If data is empty, return an empty string\n",
    "                return \"\"\n",
    "        else:  # If data is not a list, return it as-is\n",
    "            return data_str\n",
    "    except (ValueError, SyntaxError):  # If literal_eval fails, return the data as-is\n",
    "        return data_str\n",
    "\n",
    "# Convert strings to bullet points or leave as single string\n",
    "df['ideas'] = df['ideas'].apply(bullet_list_or_string)\n",
    "df['concerns'] = df['concerns'].apply(bullet_list_or_string)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df.to_excel('summaries_and_submissions.xlsx', index=False)\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee48ec",
   "metadata": {},
   "source": [
    "## Step five: visualising the summaries\n",
    "The final step aims to experiment with different visualizations of data, such as bar charts to show the average sentiment for different stakeholder groups and separate word clouds for extracted text, summaries, ideas, and concerns. This will help in getting insights from the summarized content in a graphical manner.\n",
    "\n",
    "The current visualization implements a bar chart for the average sentiment per author category and word clouds for Summary, Ideas, and Concerns texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725c7b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "\n",
    "# Excel file path\n",
    "file_path = f\"summaries_and_submissions_v2.xlsx\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Aggregating average sentiment for each author category\n",
    "avg_sentiment = df.groupby('category')['sentiment'].mean()\n",
    "\n",
    "# Creating bar chart\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(avg_sentiment.index, avg_sentiment.values)\n",
    "plt.xlabel('Author Category')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.title('Average Sentiment per Author Category')\n",
    "plt.show()\n",
    "\n",
    "# WordCloud generation function\n",
    "def generate_wordcloud(text_series, title):\n",
    "    # Convert all values in the series to strings before joining\n",
    "    text = ' '.join(text_series.astype(str))\n",
    "    wordcloud = WordCloud(width=800, height=400, max_words=100).generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for 'Summary', 'Ideas', and 'Concerns'\n",
    "generate_wordcloud(df['summary'], 'Summary Word Cloud')\n",
    "generate_wordcloud(df['ideas'], 'Ideas Word Cloud')\n",
    "generate_wordcloud(df['concerns'], 'Concerns Word Cloud')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd325ae1",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535b45b",
   "metadata": {},
   "source": [
    "## Function experiment\n",
    "\n",
    "*In progress. Experimental code. Experimenting with new ability to call function using OpenAI's API.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"constants.env\")) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Create summariser function\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"submission_summariser\",\n",
    "        \"description\": \"Summarise submissions that you are provided as if you are a highly experienced, knowledgeable Australian Government policy adviser who is known for their clear thinking and writing, outputting in valid JSON format.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"summary\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Provide an overall summary of the submission, highlighting key points.\",\n",
    "                },\n",
    "                \"ideas\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Extract the key policy ideas, if any, raised in the submission.\",\n",
    "                },\n",
    "                \"concerns\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Extract the key concerns, if any, raised in the submission.\", \n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Categorise the author as 'individual' (including more than one individual), 'government', 'business', 'other' or 'unsure'. No commentary, just the category.\",\n",
    "                },\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"On a scale of 1 to 10, rank the sentiment of the text towards the Government's current policy settings or government's proposals (as opposed to proposals suggested by the author or someone else) (1 being very negative and 10 being very positive).\"\n",
    "                }\n",
    "            \n",
    "        }\n",
    "        }\n",
    "    }    \n",
    "]\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4-0613\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        functions = functions,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('submission_data_test.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Load the data from the database\n",
    "c.execute(\"SELECT * FROM submissions\")\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Create a new table for the summaries if it doesn't exist\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS summariestest123\n",
    "    (file_name TEXT PRIMARY KEY, summary TEXT, ideas TEXT, concerns TEXT, \n",
    "     category TEXT, sentiment TEXT, page_count INTEGER, text TEXT, \n",
    "     manual_summarisation INTEGER DEFAULT 0, model_used TEXT, LLM_response TEXT)\n",
    "''')\n",
    "\n",
    "# Loop over the rows in the fetched data\n",
    "for row in rows:\n",
    "    file_name, page_count, word_count, text, tokens, manual_summarisation = row\n",
    "\n",
    "    # Check if summary already exists in the database\n",
    "    c.execute(\"SELECT file_name FROM summaries WHERE file_name = ?\", (file_name,))\n",
    "    if c.fetchone() is not None:\n",
    "        continue  # Skip this row if it's already in the database\n",
    "        \n",
    "    # Check for manual summarisation requirement\n",
    "    if manual_summarisation == 1:\n",
    "        model_used = \"Manual summarisation required\"\n",
    "        summary = ideas = concerns = category = sentiment = \"Manual summarisation required\"\n",
    "    else:\n",
    "\n",
    "        # Determine the model to use based on the number of tokens\n",
    "        model = \"gpt-3.5-turbo-16k-0613\" if tokens > 7100 else \"gpt-4-0613\"\n",
    "    \n",
    "        # Summarize the text\n",
    "        prompt = f\"\"\"\n",
    "        Your task is to summarise the following text and produce JSON output: [{text}]\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = get_completion(prompt)\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            print(f\"Error for file {file_name}: {error_message}\")\n",
    "            c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)\",\n",
    "                  (file_name, 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', page_count, text, model, error_message))\n",
    "            continue\n",
    "\n",
    "        # Parse the response as JSON\n",
    "        try:\n",
    "            response_json = json.loads(response)\n",
    "            # Extract the fields\n",
    "            summary = response_json.get('summary', 'Error: could not parse response')\n",
    "            ideas = json.dumps(response_json.get('ideas', 'Error: could not parse response'))\n",
    "            concerns = json.dumps(response_json.get('concerns', 'Error: could not parse response'))\n",
    "            category = response_json.get('category', 'Error: could not parse response')\n",
    "            sentiment = response_json.get('sentiment', 'Error: could not parse response')\n",
    "        except json.JSONDecodeError:\n",
    "            \n",
    "            # If parsing fails, ask LLM to convert to JSON\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "                Your task is to flawlessly convert the following input to valid JSON that contains the following keys: summary, ideas, concerns, category, sentiment.\n",
    "                Here is the input you must convert to valid JSON: {response}\"\"\"\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                print(f\"Error for file {file_name}: {error_message}\")\n",
    "                c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)\",\n",
    "                    (file_name, 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', 'Error: Could not generate OpenAI response', page_count, text, model, error_message))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = get_completion(prompt)\n",
    "                \n",
    "                # Extract the fields\n",
    "                summary = response_json.get('summary', 'Error: could not parse response')\n",
    "                ideas = json.dumps(response_json.get('ideas', 'Error: could not parse response'))\n",
    "                concerns = json.dumps(response_json.get('concerns', 'Error: could not parse response'))\n",
    "                category = response_json.get('category', 'Error: could not parse response')\n",
    "                sentiment = response_json.get('sentiment', 'Error: could not parse response')\n",
    "            except json.JSONDecodeError:\n",
    "                 summary = ideas = concerns = category = sentiment = 'Error: could not parse response'\n",
    "                       \n",
    "        # Insert into database\n",
    "        c.execute(\"INSERT INTO summaries VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)\",\n",
    "            (file_name, summary, ideas, concerns, category, sentiment, page_count, text, model, response))\n",
    "    \n",
    "        print(f'Summarised and stored summary for: {file_name}')\n",
    "\n",
    "# Commit the changes and close the connection to the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b90b0",
   "metadata": {},
   "source": [
    "## Prompt library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7281d4",
   "metadata": {},
   "source": [
    "### Original prompt - generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_itself = f\"\"\"\n",
    "Your task is to perform the following actions as if you are a highly experienced, knowledgeable Australian Government policy adviser who is known for their clear thinking and writing:\n",
    "1. Summarise the following public submission extracted below.\n",
    "2. Extract the key policy ideas, if any, raised in the submission.\n",
    "3. Extract the key concerns, if any, raised in the submission.\n",
    "4. Categorise the author as 'individual' (including more than one individual), 'government', 'business', 'other' or 'unsure'. No commentary, just the category. \n",
    "5. On a scale of 1 to 10, rank the sentiment of the text towards the Government's current policy settings or government's proposals (as opposed to proposals suggested by the author or someone else) (1 being very negative and 10 being very positive). No need for commentary, just the number.\n",
    "6. Output valid JSON that contains the following keys corresponding to the above tasks: summary, ideas, concerns, category, sentiment.\n",
    "Only produce output for step 6 - the JSON output should incorporate the previous answers.\n",
    "\n",
    "Here is the submission text:\"\"\"\n",
    "prompt = prompt_itself + text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4fb8e",
   "metadata": {},
   "source": [
    "### Prompt - public submissions re merger authorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_itself = f\"\"\"\n",
    "Your task is to perform the following actions as if you are a highly experienced, knowledgeable Australian Government policy adviser who is known for their clear thinking and writing:\n",
    "1. Summarise the following public submission extracted below. Make sure to highlight any key points raised in the submission about whether the proposed acquisition would substantially lessen competition in the markets for the following products: home loans; transaction accounts; deposit products; agribusiness banking; SME banking.\n",
    "2. Extract the key areas of support for the acquisition, if any, raised in the submission.\n",
    "3. Extract the key concerns about the acquisition, if any, raised in the submission.\n",
    "4. Categorise the author as 'individual' (including more than one individual), 'government', 'business', 'other' or 'unsure'. No commentary, just the category. \n",
    "5. On a scale of 1 to 10, rank the sentiment of the text towards whether the proposed acquisition should be authorised (1 being very negative and 10 being very positive). No need for commentary, just the number.\n",
    "6. Output valid JSON that contains the following keys corresponding to the above tasks: summary, ideas, concerns, category, sentiment.\n",
    "Only produce output for step 6 - the JSON output should incorporate the previous answers.\n",
    "\n",
    "Here is the submission text:\"\"\"\n",
    "    \n",
    "prompt = prompt_itself + text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de564e4c",
   "metadata": {},
   "source": [
    "### Chunking prompts v 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_prompt = f\"\"\"\n",
    "You have been given a chunk from a longer document that exceeds token limits.\n",
    "Your task is to extract and summarise useful information that, when combined with summaries of other chunks, will allow you to perform the following task. You must output in string format and finish the summary with the following \"End of chunk.\".\n",
    "The following is the prompt you would have received if the document was within token limits as well as an extract of the relevant chunk:\"\"\"\n",
    "\n",
    "prompt = chunk_prompt + prompt_itself + chunk\n",
    "\n",
    "combined_summary_prompt = f\"\"\"\n",
    "Your task is to use the following text which is a collection of summarised chunks from a longer document to complete the following task.\n",
    "The reason the document has been chunked is because it exceeded token limits.\n",
    "The following is the prompt you would have received if the document was within token limits as well as the combined summarised chunks:\"\"\"\n",
    "\n",
    "prompt = combined_summary_prompt + prompt_itself + chunk_summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
